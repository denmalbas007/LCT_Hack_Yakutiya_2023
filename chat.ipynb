{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2eb333a-fbd8-4f90-897d-f28f0cb4d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"/home/leffff/PycharmProjects/LCT_Hack_Yakutiya_2023/venv/lib/python3.10/site-packages\")\n",
    "\n",
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "\n",
    "MODEL_NAME = \"IlyaGusev/saiga_mistral_7b\"\n",
    "DEFAULT_MESSAGE_TEMPLATE = \"<s>{role}\\n{content}</s>\"\n",
    "DEFAULT_RESPONSE_TEMPLATE = \"<s>bot\\n\"\n",
    "DEFAULT_SYSTEM_PROMPT = \"Ты — Сайга, русскоязычный автоматический ассистент. Ты разговариваешь с людьми и помогаешь им.\"\n",
    "# Ты Помогаешь жителям якутии и отвечаеншь на запросы жителей России о планах развития региона.\"\n",
    "\n",
    "\n",
    "class Conversation:\n",
    "    def __init__(\n",
    "        self,\n",
    "        message_template=DEFAULT_MESSAGE_TEMPLATE,\n",
    "        system_prompt=DEFAULT_SYSTEM_PROMPT,\n",
    "        response_template=DEFAULT_RESPONSE_TEMPLATE\n",
    "    ):\n",
    "        self.message_template = message_template\n",
    "        self.response_template = response_template\n",
    "        self.messages = [{\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt\n",
    "        }]\n",
    "\n",
    "    def add_user_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def add_bot_message(self, message):\n",
    "        self.messages.append({\n",
    "            \"role\": \"bot\",\n",
    "            \"content\": message\n",
    "        })\n",
    "\n",
    "    def get_prompt(self, tokenizer):\n",
    "        final_text = \"\"\n",
    "        for message in self.messages:\n",
    "            message_text = self.message_template.format(**message)\n",
    "            final_text += message_text\n",
    "        final_text += DEFAULT_RESPONSE_TEMPLATE\n",
    "        return final_text.strip()\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt, generation_config):\n",
    "    data = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    data = {k: v.to(model.device) for k, v in data.items()}\n",
    "    output_ids = model.generate(\n",
    "        **data,\n",
    "        do_sample=True,\n",
    "        top_p=0.99,\n",
    "        top_k=0,\n",
    "        bos_token_id = 1,\n",
    "        eos_token_id = 2,\n",
    "        pad_token_id = 0,\n",
    "        max_new_tokens=128, \n",
    "    )[0]\n",
    "    print(data[\"input_ids\"].device, model.device)\n",
    "    output_ids = output_ids[len(data[\"input_ids\"][0]):]\n",
    "    output = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
    "    return output.strip()\n",
    "\n",
    "config = PeftConfig.from_pretrained(MODEL_NAME)\n",
    "# config.top_k = 1\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "# generation_config.max_new_tokens = 512\n",
    "# generation_config.top_k = 1\n",
    "print(generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d841e87-887a-4a27-85d7-1c18ee41727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inputs = [\n",
    "    # \"Планируется ли постройка много этажных гаражей, снос старых?\", \n",
    "    # \"Ответь на вопрос о развитии региона от лица мерии: За магазином Восток по Кирова ежегодно собирается большая лужа, стока воды нет, уходит под дома, боимся размытия сваи близ лежащих домов.\"\n",
    "    # \"Продолжится ли озеленение насыпей (искусственных гор) в близи города. Мне кажется, что это обязанность АЛРОСА?\"\n",
    "    \"Почему так мало, а практически никак не озеленяют наш город, не озеленяют придомовые территории новых домов...Почему сейчас всё в приоритете стоянки для машин, а не площадки для детей, не безопасность жильцов и не зелёные насаждения. Знаю многие видя, что даже проявив свою инициативу, желая посадить самим около дома кусты или деревья…соседи выбегают и говорят, что против... И 2 вопрос. Вместо снесенных домов напротив магазина Темп, можно ли оставить все насаждения многолетние...и обыграть как маленький парк, но никак не автостоянки устраивать. Очень не хочется жить в каменных джунглях.\"\n",
    "]\n",
    "\n",
    "for inp in inputs:\n",
    "    conversation = Conversation()\n",
    "    conversation.add_user_message(inp)\n",
    "    prompt = conversation.get_prompt(tokenizer)\n",
    "\n",
    "    output = generate(model, tokenizer, prompt, generation_config)\n",
    "    print(inp)\n",
    "    print(output)\n",
    "    print()\n",
    "    print(\"==============================\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
